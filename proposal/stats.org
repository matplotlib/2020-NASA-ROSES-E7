* extract PR / issue stats from GH

** ghapi way
This only sort of works
#+begin_src python
  import matplotlib
  import pandas as pd
  from pathlib import Path
  with open(Path('~/.ghoauth').expanduser(), 'r') as fin:
      token = fin.read()
  from ghapi.all import GhApi, pages
  from fastcore.net import HTTP403ForbiddenError
  from urllib.error import HTTPError
  import time
  import datetime
  api = GhApi(owner='matplotlib', repo='matplotlib', token=token)

  def get_all_the_issues(api_call):
      api = api_call.client
      cooloff = 90
      N = 10
      for j in range(N):
          try:
              # hit the API once to get to the right last page
              _ = api_call(state='all')
              print(f"{api.last_page()=}")
              if api.last_page() == 0:
                  continue
              issues = pages(api_call, api.last_page(), state='all').concat()
          except (HTTP403ForbiddenError, HTTPError) as e:
              print(e)
              # for reasons that are unclear this can produces 403
              # errors sometimes it works if you give it time and hit it
              # again?!  Assume for very old pages GH does not keep them
              # up and has to regenerate, that times out which ends up
              # coming back as a 403?
              time.sleep(cooloff)
          else:
              return issues
      else:
          raise TimeoutError(f'failed to get all of {api_call} in {N} tries with {cooloff}')

  # sometimes this takes a while, if at first it fails, try again!
  all_pulls = get_all_the_issues(api.pulls.list)
  all_issues_and_pulls = get_all_the_issues(api.issues.list_for_repo)



#+end_src
** gidgethub way
#+begin_src python
  import aiohttp
  import gidgethub
  import gidgethub.aiohttp
  import re
  import pymongo
  import asyncio

  conn = pymongo.MongoClient()
  db = conn.get_database('mpl_info')
  col = db.get_collection('log')
  m_cache = db.get_collection('cache')
  issues = db.get_collection('issues')


  def dump_cache(cache, m_cache):
      for k, v in cache.items():
          m_cache.replace_one({'k': k}, {'k': k, 'v': v}, upsert=True)

  def restore_cache(cache, m_cache):
      for doc in m_cache.find():
          cache[doc['k']] = doc['v']


  try:
      with open(os.path.expanduser('~/.ghoauth'), 'r') as f:
          oauth_token = f.read()
  except FileNotFoundError:
      oauth_token = None

  try:
      cache
  except NameError:
      cache = {}
      restore_cache(cache, m_cache)

  async def get_issues(org, repo, oauth_token):
      async with aiohttp.ClientSession() as session:
          gh = gidgethub.aiohttp.GitHubAPI(session, 'tacaswell',
                                           oauth_token=oauth_token, cache=cache)
          data = []
          async for d in gh.getiter(f"/repos/{org}/{repo}/issues{{?state}}",
                                    {'state': 'all'}):
              data.append(d)

          dump_cache(cache, m_cache)
          return data

  async def get_new_contributor_prs(org, repo, oauth_token):
      async with aiohttp.ClientSession() as session:
          gh = gidgethub.aiohttp.GitHubAPI(session, 'tacaswell',
                                           oauth_token=oauth_token, cache=cache)
          data = []
          async for d in gh.getiter(f"/repos/{org}/{repo}/issues{{?state}}",
                                    {'state': 'open'}):
              if (d['author_association'] in {'CONTRIBUTOR', 'FIRST_TIME_CONTRIBUTOR'} and
                  'pull_request' in d):

                  data.append(d)
              if len(data) > 10:
                  break

          dump_cache(cache, m_cache)
          return data

  #d_issues = await get_issues('matplotlib', 'matplotlib', oauth_token)
  # for d in new_issues:
  #    print(f"{d['user']['login']: <20} {d['author_association']: <24} {d['pull_request']['url']}")

#+end_src
** caching


#+begin_src python
  import json

  with open('/tmp/gh_dump.json', 'w') as fout:
      json.dump(d_issues, fout)
#+end_src

#+begin_src python
  import json

  with open('/tmp/gh_dump.json', 'r') as fin:
      d_issues = json.load(fin)
#+end_src


** plotting

#+begin_src python
  def split_issuse_from_pr(all_issues):
      issues = []
      prs = []

      for issue in all_issues:
          if 'pull_request' in issue:
              prs.append(issue)
          else:
              issues.append(issue)

      return issues, prs


  issues, prs = split_issuse_from_pr(d_issues)
#+end_src

#+begin_src python
  import matplotlib.pyplot as plt
  import pandas as pd

  def plot_by(gh_issues, *, ax=None, freq="M", label, show_net=True, show_rolling=False):

      opened = pd.Series(
          1,
          index=[
              datetime.datetime.strptime(p["created_at"], "%Y-%m-%dT%H:%M:%SZ")
              for p in gh_issues
          ],
      )
      closed = pd.Series(
          -1,
          index=[
              datetime.datetime.strptime(p["closed_at"], "%Y-%m-%dT%H:%M:%SZ")
              for p in gh_issues
              if p["state"] != "open"
          ],
      )
      all_at = pd.concat([closed, opened])
      opened_by, closed_by = map(
          lambda x: x.groupby(pd.Grouper(freq=freq)), (opened, closed)
      )
      if ax is None:
          fig, ax = plt.subplots()
      if show_net:
          all_at.sort_index().cumsum().plot(lw=2, ax=ax, label=f"net open {label}")
      (close_step,) = ax.step(
          closed_by.sum().index,
          closed_by.sum().values,
          where="pre",
          label=f"{label}s closed/{freq}",
      )
      (open_step,) = ax.step(
          opened_by.sum().index,
          opened_by.sum().values,
          where="pre",
          label=f"{label} opened/{freq}",
      )
      if show_rolling:
          for data, step in [(opened_by, open_step), (closed_by, close_step)]:
              ax.plot(
                  data.sum().rolling(3).mean().index,
                  data.sum().rolling(3).mean().values,
                  color=step.get_color(),
                  lw=2,
              )

      ax.legend()
#+end_src

#+begin_src python
  import matplotlib
  import matplotlib.pyplot as plt
  import numpy as np


  data = {
      "Pull Requests opened": [1656, 1752],
      "Pull Requests closed": [1613, 1775],
      "Issues opened": [795, 1108],
      "Issues closed": [756, 999],
  }
  labels = ["2019", "2020"]

  x = np.arange(len(labels))  # the label locations
  width = 0.35  # the width of the bars

  fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, sharey=True)

  for ax, typ in zip((ax1, ax2),["Pull Requests", "Issues"]):
      open_lab = f"{typ} opened"
      close_lab = f"{typ} closed"
      rects2 = ax.bar(
          x - width / 2,
          data[open_lab],
          width,
          label='opened',
          color="k",
          edgecolor="w",
          hatch="/",
      )
      rects1 = ax.bar(
          x + width / 2,
          data[close_lab],
          width,
          label='closed',
          color="w",
          edgecolor="k",
          hatch="\\",
      )
      ax.bar_label(rects1, padding=3)
      ax.bar_label(rects2, padding=3)
      ax.set_xlabel("Year")
      ax.set_title(typ)
      ax.set_xticks(x)
      ax.set_xticklabels(labels)
      ax.spines['top'].set_visible(False)
      ax.spines['right'].set_visible(False)
  ax2.legend(ncol=2)

  ax1.set_ylabel("#")
  ax1.set_ylim(top=2000)
  fig.set_size_inches(6.3, 2.75)

  plt.show()
  # ax2.bar(['2019', '2020'], data['issue closed'], color='C2', width=.7)
  # ax2.bar(['2019', '2020'], data['issue opened'], color='C3', width=.5)
#+end_src
